{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HA1 - Cats and dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://lghttp.32478.nexcesscdn.net/80E972/organiclifestylemagazine/wp-content/uploads/2015/10/Cats-and-Dogs.jpg\" alt=\"Cats and dogs\" style=\"width: 5000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this home assignment, we'll use the Kaggle dataset for the [Dogs vs. Cats competition](https://www.kaggle.com/c/dogs-vs-cats). It comprises of 25k colored images of dogs and cats. Our goal with this dataset will be to create a classifier that can tell us if the input image is of a cat or a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "- Whenever we ask you to plot anything, be sure to add a title and label the axes. If you're plotting more than one curve in the same plot, also add a legend.\n",
    "- When we ask you to train an architecture, train it for a reasonable number of epochs. \"Reasonable\" here means you should be fairly confident that training for a higher number of epochs wouldn't impact your conclusions regarding the model's performance.\n",
    "\n",
    "Tips:\n",
    "- If you get errors saying you've exhausted the GPU resources, well, then you exhausted the GPU resources ;). However, sometimes that's because TensorFlow didn't release a part of the GPU's memory. If you think your CNN should fit in your memory during training, try restarting the kernel and directly training only that architecture.\n",
    "- Every group has enough credits on google cloud to complete this assignment. However, this statement assumes you'll use your resources judiciously (e.g. always try the code first in your machine and make sure everything works properly before starting your instances) and **won't forget to stop your instance after using it,**  otherwise you might run out of credits.\n",
    "- Before starting, take a look at the images we'll be using. This is a hard task, don't get discouraged if your first models perform poorly (several participants in the original competition didn't achieve an accuracy higher than 60%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Imports\n",
    "\n",
    "In the following cell, add all the imports you'll use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The package for importing the dataset (already provided by Keras)\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Packages for defining the architecture of our model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, BatchNormalization,Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "\n",
    "# One-hot encoding\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Callbacks for training\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "# Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Ndarray computations\n",
    "import numpy as np\n",
    "\n",
    "# Confusion matrix for assessment step\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Loading the data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to head to the [Kaggle website for the cats and dogs competition](https://www.kaggle.com/c/dogs-vs-cats) and download the data from there. You should download both the `train.zip` and `test.zip` files. The split ratio between training and validation has not been made, you'll need to do it yourself. The `test.zip` file contains unlabeled data, so that participants in the contest are not able to train on this set.\n",
    "\n",
    "As mentioned in CL3, for this assignment you should use [data generators](https://keras.io/preprocessing/image/) to load the images to your CPU/GPU memory. Because of this, your folder structure for the data should conform to the folder structure expected by the data generators (i.e. the samples should be separated into one folder for each class). Furthermore, we ask you to first start with a smaller subset of the data (1/5 of the number of samples), in order to test different models faster.\n",
    "\n",
    "This means that you should create a folder structure that resembles the following (obviously, the folder names are up to you):\n",
    "\n",
    "\n",
    "         small_train             small_val                train                   val\n",
    "              |                      |                      |                      |\n",
    "              |                      |                      |                      |\n",
    "        -------------          -------------          -------------          -------------\n",
    "        |           |          |           |          |           |          |           |\n",
    "        |           |          |           |          |           |          |           |\n",
    "      Cats        Dogs       Cats        Dogs       Cats        Dogs       Cats        Dogs\n",
    "\n",
    "The `small_train` and `small_val` folders have the training and validation samples for your smaller subset of the data, while the `train` and `val` folders contain all the samples you extracted from Kaggle's `train.zip`. We provide you a notebook that shows how to achieve this, starting from the original `train` folder extracted from `train.zip`. If you do use that notebook, we encourage you to understand how each step is being done, so you can generalize this knowledge to new datasets you'll encounter.\n",
    "\n",
    "Although we specifically guide you to use 1/5 of the data in the smaller dataset, you should decide how to split the validation data. Please specify your splits in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ------------------  Smaller subset ------------------ **\n",
    "\n",
    "** % Samples in the training set:** 70\n",
    "\n",
    "** % Samples in the validation set:** 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ------------------  Entire dataset ------------------ **\n",
    "\n",
    "** % Samples in the training set:** 95\n",
    "\n",
    "** % Samples in the validation set:** 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Once you have the expected folder structure, create two data generators for automatically generating batches from the images in your smaller subset of data. Don't use any [data augmentation](https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/), but feel free to preprocess the data as you see fit. After instantiating them, run the `flow_from_directory` method with the desired arguments.\n",
    "\n",
    "Hints:\n",
    "- The specified `batch_size` should be chosen so that your don't run out of memory.\n",
    "- When feeding the images to your CNN, you'll probably want all of them to have the same spatial size, even though the .jpeg files differ in this. If so, take a look at the argument `target_size` for the `flow_from_directory` method of data generators.\n",
    "- Resizing the images to a smaller size while loading them can be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3500 images belonging to 2 classes.\n",
      "Found 1500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#########################################\n",
    "train_path = './train'\n",
    "val_path = './val'\n",
    "small_train_path = './small_train'\n",
    "small_val_path = './small_val'\n",
    "#########################################\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size=32\n",
    "target_size=(150, 150)\n",
    "\"\"\"train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "val_generator= train_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\"\"\"\n",
    "small_train_generator = train_datagen.flow_from_directory(\n",
    "        small_train_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "small_val_generator= train_datagen.flow_from_directory(\n",
    "        small_val_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your first CNN architecture for this task. Start with something as simple as possible, that you're almost sure can get an accuracy better than 50% (we'll improve upon it later).\n",
    "\n",
    "Tip:\n",
    "- If Tensorflow is your backend, your `input_shape` is always `(img_width, img_height, 3)` (i.e. channels **last**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    num_classes=2\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(10, (3, 3), input_shape=(150, 150, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(10, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model using the `fit_generator` method and the two data generators you created earlier. Train for a reasonable amount of epochs, so as to get a good sense of how well this architecture performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=base_model()\n",
    "historyBase = model.fit_generator(\n",
    "            small_train_generator,\n",
    "            steps_per_epoch=3500 // batch_size,\n",
    "            epochs=10,\n",
    "            validation_data=small_val_generator,\n",
    "            validation_steps=1500 // batch_size)\n",
    "model.save_weights('third_try.h5') \n",
    "np.save('historyBase1',historyBase.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one figure with two axes. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets.\n",
    "\n",
    "Hint:\n",
    "- The `fit_generator` method returns a `history` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_base=np.load('historyBase1').item()\n",
    "# summarize history for loss\n",
    "plt.plot(hist_base['loss'])\n",
    "plt.plot(hist_base['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these, what would you suggest for improving your model? Why?\n",
    "\n",
    "**Your answer**: Since the training loss continues to decrease after the validation  loss converges it seems that we have an overfitting problem. We will try to use some reqularisation technique to combat this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Improving your initial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve your initial model according to you answer above. Write the new definition in the cell below and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "def improved_model():\n",
    "    num_classes=2\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(10, (3, 3), input_shape=(150, 150, 3), activation='relu',kernel_regularizer=regularizers.l2(0.01),bias_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(10, (3, 3), activation='relu',kernel_regularizer=regularizers.l2(0.01),bias_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "model=improved_model()\n",
    "historyImproved = model.fit_generator(\n",
    "            small_train_generator,\n",
    "            steps_per_epoch=3500 // batch_size,\n",
    "            epochs=10,\n",
    "            validation_data=small_val_generator,\n",
    "            validation_steps=1500 // batch_size)\n",
    "model.save_weights('improved_1_try.h5')  \n",
    "np.save('historyImproved1',historyImproved.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracy in the test and validation set, using the initial model and your newly improved one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "hist_base=np.load('historyBase1.npy').item()\n",
    "hist_improved=np.load('historyImproved1.npy').item()\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(hist_base['acc'],'-g',label='Train acc base')\n",
    "plt.plot(hist_improved['acc'],'-b',label='Train acc improved')\n",
    "plt.plot(hist_base['val_acc'],'--g',label='Validation acc base')\n",
    "plt.plot(hist_improved['val_acc'],'--b',label='Validation acc improved')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your results improve? Explain why, or why not.\n",
    "\n",
    "**Your answer**: In the plot above we can see that we slightly improve the accuracy in the validation set which implies that our improved model generalises better to unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Obtaining the *best* model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to improve your model architecture by comparing the value of the metrics you're interested in both the training and validation set. Try different ideas, and consider comparing them using tensorboard. When you're happy with one architecture, copy it in the cell below and train it here. Save the optimization history (i.e. the `history` object returned by the `fit_generator`). You'll use this later to compare your best model with the one using transfer learning.\n",
    "\n",
    "**Note**: When trying different ideas, you'll end up with several different models. However, when submitting your solutions to ping-pong, the cell below must contain only the definition and training of *one* model. Remove all code related to the models that were not chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "def best_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(10, (3, 3), input_shape=(150, 150, 3), activation='relu',kernel_regularizer=regularizers.l2(0.01),bias_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(10, (3, 3), activation='relu',kernel_regularizer=regularizers.l2(0.01),bias_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model=best_model()\n",
    "historyBest = model.fit_generator(\n",
    "            small_train_generator,\n",
    "            steps_per_epoch=3500 // batch_size,\n",
    "            epochs=5,\n",
    "            validation_data=small_val_generator,\n",
    "            validation_steps=1500 // batch_size)\n",
    "model.save_weights('best_1_try.h5')  \n",
    "np.save('historyBest1',historyBest.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one figure with two axes. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Save your model](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) to disk as a HDF5 file (the architecture, weights and optimizer state). This is simply so you can use it again easily in the later parts of the notebook, without having to keep it in memory or re-training it. The actual `.h5` files you create are not relevant to your ping-pong submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of trying to come up with a good architecture for this task, we'll use the VGG16 architecture, but with the top layers removed (the fully connected + classification layers). We'll substitute them with a single fully connected layer, and a classification layer that makes sense for our problem.\n",
    "\n",
    "However, this model has a very high capacity, and will probably suffer a lot from overfitting if we try to train it from scratch, using only our small subset of data. Instead, we'll start the optimization with the weights obtained after training VGG16 on the ImageNet dataset.\n",
    "\n",
    "Start by loading the VGG16 model without the top layers, from the `applications` submodule from Keras. Make sure to also load the weights obtained from the ImageNet pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "image_size=150\n",
    "#Load the VGG model\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new model with the layers you want to add on top of VGG. The kernels and bias in these layers should be initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32,kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform', activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1,kernel_initializer='random_uniform',\n",
    "                bias_initializer='random_uniform', activation='sigmoid'))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the new model on top of VGG.\n",
    "\n",
    "Tip:\n",
    "- The VGG model you loaded from the `applications` submodule is from the [`Model`](https://keras.io/models/model/) class, not the `Sequential` class, so it doesn't have some methods you're used to (like `add`, for instance). It might be helpful to read [this introduction to the Model class](https://keras.io/getting-started/functional-api-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 262209    \n",
      "=================================================================\n",
      "Total params: 14,976,897\n",
      "Trainable params: 14,976,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "modelFull = models.Sequential()\n",
    "# Add the vgg convolutional base model\n",
    "modelFull.add(vgg_conv)\n",
    "modelFull.add(model)\n",
    "modelFull.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Using VGG features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're almost ready to train the new model. However, since the top layers of this architecture are being initialized randomly, it's sometimes possible for them to generate large gradients that can wreck the pretraining of the bottom layers. To avoid this, freeze all the VGG layers in your architecture (i.e. signal to the optimizer that these should not be changed during optimization) by setting the `trainable` attribute of them to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.training.Model object at 0x0000015B35E087F0> False\n",
      "<keras.engine.sequential.Sequential object at 0x0000015B3973FAC8> True\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 262209    \n",
      "=================================================================\n",
      "Total params: 14,976,897\n",
      "Trainable params: 262,209\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Freeze the layers except the last  layers\n",
    "for layer in modelFull.layers[:1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in modelFull.layers:\n",
    "    print(layer, layer.trainable)\n",
    "modelFull.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the callbacks (if any) you would like to use for this training here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 18/109 [===>..........................] - ETA: 28:59 - loss: 0.6627 - acc: 0.5764"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-63876a4663a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msmall_val_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             validation_steps=1500 // batch_size)\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'historyVgg1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhistoryVgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\dml\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\dml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\dml\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\dml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\dml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\dml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\dml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "modelFull.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "historyVgg = modelFull.fit_generator(\n",
    "            small_train_generator,\n",
    "            steps_per_epoch=3500 // batch_size,\n",
    "            epochs=1,\n",
    "            validation_data=small_val_generator,\n",
    "            validation_steps=1500 // batch_size)\n",
    "np.save('historyVgg1',historyVgg.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one figure with two axes. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Figure' object has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fd8aa046f258>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0max_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0max_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VGG with modified top layers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Figure' object has no attribute 'title'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_VGG=np.load('historyVgg2.npy').item()\n",
    "f, (ax_loss, ax_acc) = plt.subplots(1, 2, sharey=False)\n",
    "ax_acc.plot(hist_VGG['acc'],'b',label='Train acc')\n",
    "ax_acc.plot(hist_VGG['val_acc'],'-b',label='Validation acc')\n",
    "ax_loss.plot(hist_VGG['loss'],'b',label='Train loss')\n",
    "ax_loss.plot(hist_VGG['val_loss'],'-b',label='Validation loss')\n",
    "ax_acc.set_title('Accuracy')\n",
    "ax_loss.set_title('Loss')\n",
    "ax_acc.legend(loc='best')\n",
    "ax_loss.legend(loc='best')\n",
    "f.title('VGG with modified top layers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracy in the test and validation set, using the model obtained in step 4 and the one using transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_step4=np.load('historyBest1').item()\n",
    "hist_VGG=np.load('historyVGG1').item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these results. Which approach worked best, starting from scratch or doing transfer learning? Explain how you evaluated this.\n",
    "\n",
    "**Your answer**: Transfer learning worked best. We evaluated this by comparing accuracy on **VAlidation or test, whic one** set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the main differences between the ImageNet dataset and the Dogs vs Cats dataset we used?\n",
    "\n",
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though there are considerable differences between these datasets, why is it that transfer learning is still a good idea?\n",
    "\n",
    "**Your answer**: Since the layers closer to the input detects more general features it is unecessary to train these layers since they will most likely learn the same weights (If we had as large training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which scenario would transfer learning be unsuitable?\n",
    "\n",
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a better starting point for the top layers, we can train the entire network. Unfreeze the bottom layers.\n",
    "\n",
    "Tip:\n",
    "- Always recompile your model after changing anything in it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the callbacks (if any) you would like to use for this training here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and train the model.\n",
    "\n",
    "Tip:\n",
    "- Even though we not have a decent starting point for the optimization, it's still possible that a bad hyper-parameter choice wrecks the preinitialization. Make sure to use a small learning rate for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracy in the test and validation set, using the model trained with freezed layers and the one you just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the model's performance improve? Why (why not)?\n",
    "\n",
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFull.save('modelFull.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Improving the top model (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve the architecture for the layers you add on top of VGG16. Try different ideas, and consider comparing them using tensorboard. When you're happy with one architecture, copy it in the cell below and train it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracy in the test and validation set, using the model trained in step 5.2 and the one you just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train the model that achieved the best performance so far using the entire dataset.\n",
    "\n",
    "**Note**: start the optimization with the weights you obtained training in the smaller subset, i.e. *not* from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create two new data generators, one for training samples and one for validation samples. This time, they'll load data from the folders for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the callbacks you would like to use. This optimization might take a long time, so TensorBoard is advised ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracy on the training and validation set, obtained when you trained this model on the smaller subset of data, and when you trained it now, with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you conclude from these plots?\n",
    "\n",
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll evaluate your final model, obtained in step 6, on the test set. As mentioned before, the samples in the test set are not labeled, so we can't compute any performance metrics ourselves. Instead, we'll create a .csv file containing the predictions for each sample, and submit it to Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the predictions for all samples in the test set according to your best model, and save it in a .csv file with the format expected by the competition.\n",
    "\n",
    "Tip:\n",
    "- There is a `sample_submission.csv` file available for download in the same place where you downloaded the data from. Take a look at it to better understand what is the expected format here.\n",
    "- Take a look at the .csv file you generated and make sure it's reasonable before submitting it to Kaggle. You're only allowed a few submissions per day.\n",
    "\n",
    "Hints:\n",
    "- The Python module `os` has a `listdir` function, which returns the filenames of all files in a given path.\n",
    "- If you don't know how to create and write to files with Python, Google can help.\n",
    "- Keras has a submodule called `preprocessing.image`, with some handy functions (for instance `load_img` and `img_to_array`).\n",
    "- Alternatively, it's possible to use data generators for this task as well, but be careful about the order of your predictions (if you get a bad score at Kaggle, this is probably why)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you created your submission file, submit it to Kaggle for evaluation. The [old competition](https://www.kaggle.com/c/dogs-vs-cats) does not allow submissions any more, so submit your file to the [new one](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition). Kaggle evaluates your submission according to your log-loss score. What is the score you obtained?\n",
    "\n",
    "**Your answer**: (fill in here)\n",
    "\n",
    "What was the username you used for this submission?\n",
    "\n",
    "**Your answer**: (fill in here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
